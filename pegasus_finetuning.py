import torch
import pandas as pd
import evaluate
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
from sklearn.model_selection import train_test_split

# âœ… 1ï¸âƒ£ GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device}")

# âœ… 2ï¸âƒ£ Train/Validation ë°ì´í„° ë¡œë“œ
dataset = load_dataset("parquet", data_files={
    "train": "Save/train.parquet",
    "validation": "Save/valid.parquet"
})

# âœ… 3ï¸âƒ£ Pegasus ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (QLoRA ì ìš©)
model_name = "google/pegasus-xsum"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# âœ… 4ï¸âƒ£ QLoRA ì ìš©ì„ ìœ„í•œ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # âœ… 4-bit ì–‘ìí™” ì ìš©
    bnb_4bit_compute_dtype=torch.bfloat16,  # âœ… 4-bit ì—°ì‚°ì„ ìœ„í•´ float16 ì‚¬ìš©
    bnb_4bit_use_double_quant=True  # âœ… ì´ì¤‘ ì–‘ìí™” í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
)

# âœ… 5ï¸âƒ£ ëª¨ë¸ì„ 4-bit ì–‘ìí™”ëœ í˜•íƒœë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)

# âœ… 6ï¸âƒ£ QLoRA ì„¤ì • (LoRA Adapter ì ìš©)
lora_config = LoraConfig(
    r=8,  # LoRA ë­í¬ (ì ì ˆí•œ ê°’: 4~16)
    lora_alpha=32,  # LoRA í•™ìŠµë¥  ìŠ¤ì¼€ì¼ë§
    target_modules=["q_proj", "v_proj"],  # LoRA ì ìš©í•  ë ˆì´ì–´ ì„ íƒ
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

# âœ… 7ï¸âƒ£ QLoRA ëª¨ë¸ ìƒì„±
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # âœ… í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸ (LoRA ì ìš© ì—¬ë¶€ ì²´í¬)

# âœ… 8ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ (í† í°í™”)
def preprocess_function(examples):
    model_inputs = tokenizer(examples["input"], truncation=True, padding="max_length", max_length=512)
    labels = tokenizer(examples["output"], truncation=True, padding="max_length", max_length=128)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(preprocess_function, batched=True)

# âœ… 9ï¸âƒ£ í•™ìŠµ ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë°˜ì˜)
training_args = TrainingArguments(
    output_dir="./results_qlora",  # âœ… QLoRA ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,  # âœ… ìµœê·¼ 2ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€
    per_device_train_batch_size=16,  # âœ… ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°€ëŠ¥ (VRAM ì ˆì•½)
    per_device_eval_batch_size=16,
    learning_rate=5e-4,  # âœ… QLoRAëŠ” ì¼ë°˜ì ì¸ í•™ìŠµë¥ ë³´ë‹¤ ë†’ê²Œ ì„¤ì • ê°€ëŠ¥
    num_train_epochs=10,
    weight_decay=0.01,
    logging_steps=10,
    fp16=True  # âœ… Mixed Precision í™œì„±í™”
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer
)

# âœ… 10ï¸âƒ£ QLoRA ëª¨ë¸ í•™ìŠµ ì‹œì‘!
trainer.train()

# âœ… 11ï¸âƒ£ í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ëª¨ë¸ ì €ì¥
model.save_pretrained("./final_pegasus_qlora_model")  # âœ… í•™ìŠµëœ ëª¨ë¸ ì €ì¥
tokenizer.save_pretrained("./final_pegasus_qlora_model")  # âœ… í† í¬ë‚˜ì´ì € ì €ì¥

# âœ… 12ï¸âƒ£ í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸°
def generate_summary(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512).to(device)
    output = model.generate(**inputs, max_length=128, num_beams=5)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# ğŸ¯ í…ŒìŠ¤íŠ¸ ë¬¸ì¥
test_sentence = "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë©°, ê²½ì œ, ì •ì¹˜, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ì´ë‹¤."
summary = generate_summary(test_sentence)
print("ìš”ì•½ ê²°ê³¼:", summary)

# âœ… 13ï¸âƒ£ ROUGE ì ìˆ˜ í‰ê°€
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    return {key: value.mid.fmeasure for key, value in result.items()}

# ğŸ¯ Test ë°ì´í„° í‰ê°€
test_dataset = load_dataset("parquet", data_files={"test": "Save/test.parquet"})
tokenized_test = test_dataset.map(preprocess_function, batched=True)
predictions = trainer.predict(tokenized_test["test"])

print("ROUGE ì ìˆ˜:", compute_metrics(predictions))