import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from collections import Counter
from nltk import ngrams
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

# âœ… paust/pko-t5-base ëª¨ë¸ ë° BPE ê¸°ë°˜ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "paust/pko-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def truncate_text(text, max_tokens=512):
    """ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ í† í° ê¸¸ì´ì— ë§ê²Œ ìë¥´ëŠ” í•¨ìˆ˜"""
    tokens = tokenizer.tokenize(text)
    if len(tokens) > max_tokens:
        return tokenizer.convert_tokens_to_string(tokens[:max_tokens])
    return text

def summarize_text_pko_t5(model, tokenizer, text, max_length=60):
    """pko-T5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìš”ì•½"""
    text = truncate_text(text)  # ğŸ”¥ ì…ë ¥ ê¸¸ì´ 512 ì´í•˜ë¡œ ì¤„ì´ê¸°
    input_text = "ìš”ì•½: " + text  # ğŸ”¥ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding="longest", max_length=512)

    with torch.no_grad():
        summary_ids = model.generate(
            input_ids=inputs["input_ids"], 
            attention_mask=inputs["attention_mask"], 
            max_length=max_length, 
            num_beams=7,  # ğŸ”¥ ë” ë‚˜ì€ ìš”ì•½ ì„ íƒ
            temperature=0.7,  # ğŸ”¥ ë” ë‹¤ì–‘í•œ ìš”ì•½ ìƒì„±
            early_stopping=True
        )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def f1_score(preds, targets, n=4):
    """n-gram ê¸°ë°˜ F1 Score ê³„ì‚°"""
    f1_list = []
    for i in range(1, n+1):
        f1_n = []
        for pred, target in zip(preds, targets):
            epsilon = 1e-12

            # n-gram ì¹´ìš´íŠ¸
            pred_cnt = Counter(ngrams(pred, i))
            target_cnt = Counter(ngrams(target, i))

            # BLEU@n, ROUGE@n ê³„ì‚°
            pred_all = sum(pred_cnt.values())
            target_all = sum(target_cnt.values())
            matched = 0
            for k, v in pred_cnt.items():
                if k in target_cnt:
                    matched += min(pred_cnt[k], target_cnt[k])
            bleu = matched / (pred_all + epsilon)
            rouge = matched / (target_all + epsilon)

            # F1@n ê³„ì‚°
            f1 = (2 * bleu * rouge) / (bleu + rouge + epsilon)
            f1_n.append(f1)
        f1_list.append(sum(f1_n) / len(f1_n))

    # ìµœì¢… F1-score ê³„ì‚°
    f1 = sum(f1_list) / len(f1_list)
    return f1

# âœ… í•œêµ­ì–´ ì›ë³¸ í…ìŠ¤íŠ¸
original_text = """ìœ„ì›ì¥ ì‹ í•™ìš©] "ì¢Œì„ì„ ì •ëˆí•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ì„±ì›ì´ ë˜ì—ˆìœ¼ë¯€ë¡œ ì œ309íšŒ êµ­íšŒ(ì„ì‹œíšŒ) ì œ1ì°¨ êµìœ¡ê³¼í•™ê¸°ìˆ ìœ„ì›íšŒë¥¼ ê°œì˜í•˜ê² ìŠµë‹ˆë‹¤. ìš°ì„  ì…ë²•ì¡°ì‚¬ê´€ì˜ ë³´ê³ ì‚¬í•­ì´ ìˆê² ìŠµë‹ˆë‹¤." ì…ë²•ì¡°ì‚¬ê´€ ê¹€ëŒ€í˜•] "ë³´ê³ ì‚¬í•­ì„ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. (ë³´ê³ ì‚¬í•­ì€ ëì— ì‹¤ìŒ)" ìœ„ì›ì¥ ì‹ í•™ìš©] "ì¡´ê²½í•˜ëŠ” ì„ ë°°ã†ë™ë£Œ ìœ„ì›ë‹˜ ì—¬ëŸ¬ë¶„! ì–´ì œ ë³¸íšŒì˜ì—ì„œ ì—¬ì•¼ë¥¼ ë§‰ë¡ í•˜ê³  ì €ë¥¼ êµê³¼ìœ„ì›ì¥ìœ¼ë¡œ ì„ ì„í•´ ì£¼ì‹  ë° ëŒ€í•´ì„œ ì´ ìë¦¬ë¥¼ ë¹Œë ¤ ë‹¤ì‹œ í•œë²ˆ ê°ì‚¬ì˜ ë§ì”€ë“œë¦½ë‹ˆë‹¤..."""

# ì •ë‹µ ìš”ì•½ (ì°¸ì¡° ìš”ì•½)
reference_summary = "ìœ„ì›ì¥ ì‹  ì”¨ëŠ” ê³ ì§ˆì ì¸ ê¸°ì´ˆê³¼í•™ ê¸°ë°˜ ë¶€ì‹¤ì— ì´ê³µê³„ë¥¼ í™€ëŒ€í•˜ê³  ìˆëŠ” ìš°ë¦¬ë‚˜ë¼ëŠ” êµìœ¡ê³¼ ê³¼í•™ê¸°ìˆ  ë°œì „ì— ì´ë ¥ì„ ê¸°ìš¸ì—¬ì•¼ í•œë‹¤ê³  í–ˆë‹¤."

# pko-T5ë¥¼ ì´ìš©í•œ ìš”ì•½ ìƒì„±
predicted_summary = summarize_text_pko_t5(model, tokenizer, original_text)
print("Generated Summary:", predicted_summary)

# ì •ìƒì ì¸ ìš”ì•½ì´ ìƒì„±ë˜ì§€ ì•ŠëŠ” ê²½ìš° ê²½ê³  ë©”ì‹œì§€
if len(predicted_summary.strip()) == 0 or predicted_summary in [",   .", ".", ""]:
    print("âš ï¸ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ìš”ì•½ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì…ë ¥ ê¸¸ì´ ë° ëª¨ë¸ ì„ íƒì„ í™•ì¸í•˜ì„¸ìš”.")

# F1 Score ê³„ì‚° (n-gramì„ ìœ„í•œ í† í°í™”)
predicted_tokens = word_tokenize(predicted_summary.lower())
reference_tokens = word_tokenize(reference_summary.lower())

# F1 Score ê³„ì‚°
f1 = f1_score([predicted_tokens], [reference_tokens])
print(f"F1 Score: {f1:.4f}")