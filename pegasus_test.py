import json
import torch
import nltk
from collections import Counter
from nltk import ngrams
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from transformers import BartTokenizer, BartForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import AutoTokenizer, AutoModel

# NLTK í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
nltk.download("punkt")

# 1ï¸âƒ£ JSON íŒŒì¼ ë¡œë“œ
json_path = "/home/elicer/Prometheus/Report_Summary/Report_data/REPORT-minute-00001-00001.json"  # JSON íŒŒì¼ ê²½ë¡œ
with open(json_path, "r", encoding="utf-8") as file:
    data = json.load(file)

# 2ï¸âƒ£ GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ Using device: {device}")

# 3ï¸âƒ£ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° (GPUë¡œ ì´ë™)
# model_name = "google/pegasus-large"  # ë˜ëŠ” "google/pegasus-large"
# tokenizer = PegasusTokenizer.from_pretrained(model_name)
#model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)

# Load model directly


tokenizer = AutoTokenizer.from_pretrained("gogamza/kobart-base-v2")
model = AutoModel.from_pretrained("gogamza/kobart-base-v2").to(device)

# 4ï¸âƒ£ íšŒì˜ë¡ ì›ë¬¸ ê°€ì ¸ì˜¤ê¸°
passage = data["Meta(Refine)"]["passage"]

# 5ï¸âƒ£ PEGASUS ìš”ì•½ ìƒì„± (ì…ë ¥ì„ GPUë¡œ ì´ë™)
inputs = tokenizer(passage, return_tensors="pt", truncation=True, padding="longest", max_length=512)
inputs = {k: v.to(device) for k, v in inputs.items()}  # GPUë¡œ ì´ë™

summary_ids = model.generate(**inputs)
generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# 6ï¸âƒ£ ì°¸ì¡° ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
reference_summary = data["Annotation"]["summary1"]  # summary1ì„ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©

# 7ï¸âƒ£ F1 Score ê³„ì‚° í•¨ìˆ˜ (n-gram ê¸°ë°˜)
def f1_score(preds, targets, n=4):
    f1_list = []
    for i in range(1, n+1):
        f1_n = []
        for pred, target in zip(preds, targets):
            epsilon = 1e-12

            # n-gram ì¹´ìš´íŠ¸
            pred_cnt = Counter(ngrams(pred, i))
            target_cnt = Counter(ngrams(target, i))

            # BLEU@n, ROUGE@n ê³„ì‚°
            pred_all = sum(pred_cnt.values())
            target_all = sum(target_cnt.values())
            matched = 0
            for k, v in pred_cnt.items():
                if k in target_cnt:
                    matched += min(pred_cnt[k], target_cnt[k])
            bleu = matched / (pred_all + epsilon)
            rouge = matched / (target_all + epsilon)

            # F1@n ê³„ì‚°
            f1 = (2 * bleu * rouge) / (bleu + rouge + epsilon)
            f1_n.append(f1)
        f1_list.append(sum(f1_n) / len(f1_n))

    # ìµœì¢… F1-score ê³„ì‚°
    f1 = sum(f1_list) / len(f1_list)
    return f1

# 8ï¸âƒ£ í† í°í™” ë° F1 Score ê³„ì‚°
gen_tokens = nltk.word_tokenize(generated_summary.lower())
ref_tokens = nltk.word_tokenize(reference_summary.lower())

f1_result = f1_score([gen_tokens], [ref_tokens])

# 9ï¸âƒ£ ê²°ê³¼ ì¶œë ¥
print(f"\nğŸ”¹ PEGASUS ìš”ì•½: {generated_summary}")
print(f"ğŸ”¹ ì°¸ì¡° ìš”ì•½: {reference_summary}")
print(f"\nâœ… F1 Score: {f1_result:.4f}") 